Project Paper Outline

Abstract

Introduction:
Other Studies from the survey paper
Browser Plugins
	issues: label strictly by URL rather than content, and domains are relatively easy to obtain, and easy to make look legitimate. , Signal 
	Survey of Features
	Motivation is trying to find better ways to detect fake news using text features alone. 

Methods:
	Found list of labeled URLs from open source projects, SignalMedia, Fake News, they labeled based on volunteers, there could be potential errors in labeling, and there are disputes on what things are classified as.  
	Our approach is advantageous because we consider text/title content only, however it could easily be used in conjunction with URL labels in the future. 
	Introduce our result, simple models SVM, naive Bayes, one v all schema. matrix of concatenated features. Particular shortcomings: more variables than observations, there's sparsity, we are including penalizing and predictions that will improve these models. 
	
	
Theory:
	Penalizing to improve predictions. 


While the issue of mass false news distribution has been an issue as long as print media has been around, the advent of social media has irreversably expanded the scope, reach, and speed of these misleading news stories. A particularly egregious example of this proliferation can be found in the bizarre allegations during the 2016 United States Presidential campaign where, taken from sparse and decontextualized electronic communications that had been leaked, it was highly popularized that members of the campaign staff of Hilary Clinton were involved in an illicit sexual slavery ring based out of a pizza restaurant. This falsified and sensationalized story was widely distributed through a conspiracy site known as InfoWars, and proved to be nearly fatal as a man wielding a semiautomatic weapon, entered the establishment and claimed to be "saving the children." While this is an extreme example of the effect that massively distributed "fake news," as it has been dubbed, can have, what is more contemptuous is the smaller lies and falsehoods spread by dubious sites with nearly convincing names and website domains, intended primarily for the casual glance on social media, rather than any prolonged examination. The every day to exposure to such falsehoods can have a multitude of effects, the most pressing of which are the tendencies to the extreme, i.e. either to accept all information on one's social media feed to be true, or to doubt all media and discard useful information outright. With the assumption that a free and functioning democracy is predicated on open and thoughtful discourse, it is clear that these are outcomes that one would hope to avoid. 
Our purpose with this research is to contribute to the ongoing study on how we may responsibly and effectively filter out blatantly false news sources, but additionally how we may delineate false news sources from satirical outlets, political news sites, and potentially extremely ideologically biased news sources. There have been a multitude of sociological papers on the advent of these news sites, and the motives that would lead one to believe or fabricate such content, however, in comparison relatively little has been done in the specific field of false news detection. Henceforth will be detailed a survey of papers and heretofore attempts at classification. A relatively recent and simple attempt to fight back against misleading content comes in the form of web browser plugins, such as "Fake News Detector" and the "Signal Media" plugin. These are simple plugins that operate through a list of URLs that are marked as "false", "bias", "credible", etc. and alert a user if they happen upon one of the sites. While these may do well for the most well known and established sites, they suffer in crucial areas, namely that they operate via list, and additions to the list happen through an open source project format that will not be updated as fast as fake news sites are published. A huge issue with false and misleading news sites is the ease by which a domain name can be purchased and structured to look like a credible news site. By merely examining an article's URL, rather than content, these plugins quickly become susceptible to rapid obsolescence due to their lack of currency. Therefore, a huge motivation for this project was the need for a classifier that would not only look at the URL, but also the content of the article itself before classification so that given a website that the classifier has never before encountered, a basic understanding of its characteristics can be provided. 
Conroy et al. have detailed a survey of several methods, some of which have been previously shown to be effective at classifying false statements. They describe two major categories of methods: Linguistic Approaches, which focus on content of texts and analysis to determine patterns, and the Network Approaches, where article metadata, or alternatively a knowledge network such as a database, provide aggregate deception measures. They incorporate machine learning techniques to train classifiers. So far there is no known rigorous typology of methods in current literature. 
The linguistic approach makes use of “predictive deception cues” in the content of the text. The simplest method of representing text is as a bag of words approach, where every word is significant. Individual words and n-gram frequencies can be recorded and analyzed to find cues of deception. Part of speech tagging, or shallow syntax, provide frequency sets to reveal cues. The drawback is the lack of context that comes from isolating n-grams, but this method can also be useful in tandem with others. One may also analyze deeper syntax, using Probability Context Free Grammars, which are sentences that have been converted to parse trees, with probabilities assigned. These can be used with 85-91% accuracy to detect deception. Once again, alone, these methods may not be sufficient to identify deception, and studies have combined the approach with other techniques.An extension of this method can be found in semantic analysis, wherein attribute:descriptor pairs are compared to existing analogous data, known as a content profile. For example, if the author claims that an art museum is near a hotel, this can potentially be compared to geographic data, or product/business reviews which may or may not present evidence to the contrary. This method has been shown to be 91% accurate; however, associating attributes and descriptors can be challenging without sufficient data, and finding content profile with confirmed correct information is itself a daunting task. Rhetorical Structure Theory provides an analytic framework for identifying rhetorical relations between linguistic elements. Systematic differences between true and false messages can be combined with a Vector Space Model that calculates a message vector’s distance from a truth center and a deceptive center. This model makes use of prominent rhetorical relations found in deceptive texts to be indicative of deception.
Support Vector Machines and Naïve Bayesian models are examples of classifiers that utilize sets of words and category frequencies to train them. They predict deception by using numeric clustering and distances. Naïve Bayes algorithms note accumulated evidence of correlations between one variable and the others in the model. Classification results from unintended emotional cues and syntactic evidence resulting from exaggeration of the sentiment required of deception. Network approaches can complement linguistic approaches, especially with the advent of sites such as twitter. Knowledge networks, such as DBpedia or the Google Relation Extraction Corpus, are used to analyze false “factual statements” by calculating a simple shortest path, to find semantic proximity. The shorter the path, the more likely the claim is factual. One challenge with this method is the reliance on a pre-existing knowledge base. Another network based method involves the use of metadata and “telltale behavior” of questionable sources, such as the inclusion of hyperlinks or associated metadata, which can be compiled. Centering resonance analysis can be used to link the most important words that link other words in the network.Combining network with linguistic methods adds the “trust” element by identifying source behavior beyond simply content. Additionally, data is much needed in this field, and is a worthwhile contribution on its own. 
	Many of these previously described methods have been shown to be successful in some contexts but not others, but as has been previously stated, there has not been a rigorous classification of methods in this field as of yet. Another of the main goals for the project was determining and differentiating classification methods by effectivness. Another of the biggest challenges for this field cited by Conroy et al. is the lack of published datasets for this field. To that end, we have assembled a large dataset of hundreds of thousands of articles, with associated metadata and their credibility classification to be published with our work for the benefit of the field of fake news detection. 
	Another paper by Horne and Adalt gives a very specific example of classification research in the field. They have assembled and published a dataset themselves, along with a classifier they created using numerous linguistic features to classify articles on a scale from real to fake using a linear kernel Support Vector Machine, coming to the conclusion that content widely differs between real and fake sources. Additionally, their conclusions were that the title was the most significant predictor when it came to classifying real vs. fake news, citing that false and misleading news titles are often longer and jam in many more claims than their articles even address. From this paper, many of the textual features used in our research have been obtained, detailed as follows. 
	A simple and naive linguistic approach to classification would merely use textual frequencies, such as n-grams to verify their predictive power, however, these are often heavily skewed and dependent on document size, so much so that they may not even be useful. A much more useful measure similar to this is text frequency-inverse document frequency or TF-IDF, which weights frequency by document length by logarithmically scaling text frequency in one document depending on its prevalence in the corpus. Another potentially useful measure comes in the form of sentiment analysis, wherein positive and negative words, and so-called emphatic terms are measured to classify a document in relation to a positive and negative pole. The point of this statistic is to determine an article's objectivity in presenting information, which can play a role in determining a biased source. 

Methodology: 

Perhaps the most challenging element of this effort is determining a safe and objective definition for what can be reasonably considered false and misleading news itself. To that end, we consulted existing resources on the subject; as mentioned earlier, browser plugins have already determined sites known for disseminating fallacious, as well as reliable articles and compiled them in a data structure mapping site names to categories of content they are known for publishing. The five categories we chose to examine were credible, false, biased, political, and satirical. Credible news sources are those which are generally trusted and produce relatively objective content, false denotes those which have been created intentionally to mislead the public, biased sources are ideologically bent and cover current events with an overt agenda, satire are those news sources that openly identify as publishing false content for the purposes of amusement, and political sources cover news with the purposes of promoting a political party. Many online publishers had more than one category associated with them. This research project acknowledges the difficulty in objectively determining the degree to which any one site can be known to produce content that conforms to one category or the other; however, given the open source nature of these browser plugins, we have been able to independently confirm that much deliberation and research has been put into these classifications. The intention of this project has been to improve upon the usage of URLs as classifiers through the additional usage of text features present in  both the titles and articles themselves. 
Now, given a list of publishers and categories of content that they are known to publish, we tasked  ourselves with scraping these webpages in search of articles, and many data points that could be reasonably associated with them. Using the python library BeautifulSoup ver. 4, websites were crawled through and articles were collected regularly to keep our database reasonably up to date with new content. After assembling this dataset, we collected from X sources for a total of Y articles, stored in a cloud based database. This dataset has been published alongside our research in the hopes that further work will be done in this field using it. 
Next, extensive research was done on selecting previously. Specifically, we decided upon the use of n-grams weighted through TF-IDF. Our analysis pipeline was constructed through a conversion of the article's text and titles into word vectors, which specifically requires the replacement and/or removal of non-sentential punctuation, repetitions, and tags from the text, and then lemmatizing the text, that is, removing grammatical markers from the ends of words, i.e. removing pluralization, tense etc. 
After formatting the text in this manner, a vocabulary dictionary was produced from the summation of the text documents. 

